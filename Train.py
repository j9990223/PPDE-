import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt


tf.compat.v1.reset_default_graph()
precision = tf.float32

# Set the default parameter 
# delta is the standard deviation used to initiate weight and bias matrices
num_train_pts = 1000
num_test_pts = 5000
hidden_dim = 300
depth = 11
input_dim = 2
output_dim = 121
batch_size = 250
delta = 0.1
epochs = 10000

# Load the files generated using Datagen.py
x_test = np.load("X_test.npy")
x_train = np.load("X_train.npy")
y_train = np.load("Y_train.npy")
y_test = np.load("Y_test.npy")
Gram = np.load("G.npy")

# The files generated by Datagen.py have float64 precision. However, the square root of Gram matrix has float32 precision.
# Therefore, we need to transform the data to float32 precision.
# The files also have a transposed shape, therefore we need to transpose them to fit in the neural network structure.
# I assume the gram matrix is also transposed since it is generated by the same code. So I also transpose the gram matrix.
x_test=np.transpose(x_test).astype(np.float32)
x_train=np.transpose(x_train).astype(np.float32)
y_train=np.transpose(y_train).astype(np.float32)
y_test=np.transpose(y_test).astype(np.float32)
Gram=np.transpose(Gram)

# Define the activation function used. In tensorflow, the default alpha to the leaky_relu is 0.2. 
# It is the same used in the PPDE paper. Therefore we don't need to define it.
rho = tf.nn.leaky_relu


# Define the structure of a single layer.
def default_block(x, layer, dim1, dim2, weight_bias_initializer, rho, precision=tf.float32):
    W = tf.compat.v1.get_variable(name='l' + str(layer) + '_W', shape=[dim1, dim2],
                                  initializer=weight_bias_initializer, dtype=precision)

    b = tf.compat.v1.get_variable(name='l' + str(layer) + '_b', shape=[dim2, 1],
                                  initializer=weight_bias_initializer, dtype=precision)

    return rho(tf.matmul(W, x) + b)


# Define the structure of the neural network.
def funcApprox(x, layers=11, input_dim=2, output_dim=121, hidden_dim=300, precision=tf.float32):
    print('Constructing the tensorflow nn graph')

    weight_bias_initializer = tf.random_normal_initializer(stddev=delta)

    with tf.compat.v1.variable_scope('UniversalApproximator'):
        in_W = tf.compat.v1.get_variable(name='in_W', shape=[hidden_dim, input_dim],
                                         initializer=weight_bias_initializer, dtype=precision)

        in_b = tf.compat.v1.get_variable(name='in_b', shape=[hidden_dim, 1],
                                         initializer=weight_bias_initializer, dtype=precision)

        z = tf.matmul(in_W, x) + in_b

        x = rho(z)



        for i in range(layers):
            choice = 0
            x = default_block(x, i, hidden_dim, hidden_dim, weight_bias_initializer, precision=precision,
                              rho=rho)
            choice = 1

        out_v = tf.compat.v1.get_variable(name='out_v', shape=[output_dim, hidden_dim],
                                          initializer=weight_bias_initializer, dtype=precision)

        out_b = tf.compat.v1.get_variable(name='out_b', shape=[output_dim, 1],
                                          initializer=weight_bias_initializer, dtype=precision)

        z = tf.math.add(tf.linalg.matmul(out_v, x, name='output_vx'), out_b, name='output')
        return z

    
# Define a function that generates random batches.    
def get_batch(X_in, Y_in, batch_size):
    X_cols = X_in.shape[0]
    Y_cols = Y_in.shape[0]

    for i in range(X_in.shape[1]//batch_size):
        idx = i*batch_size + np.random.randint(0,10,(1))[0]

        yield X_in.take(range(idx,idx+batch_size), axis = 1, mode = 'wrap').reshape(X_cols,batch_size), \
              Y_in.take(range(idx,idx+batch_size), axis = 1, mode = 'wrap').reshape(Y_cols,batch_size)

#Reset to default before training the variables. 
tf.compat.v1.reset_default_graph()
tf.compat.v1.disable_eager_execution()

# Start the training
with tf.compat.v1.variable_scope('Graph') as scope:
    
# We define placeholders, so x and y_true can be automatically selected from a directory.    
    x = tf.compat.v1.placeholder(precision, shape=[2, None], name='input')
    y_true = tf.compat.v1.placeholder(precision, shape=[121, None], name='y_true')

    y = funcApprox(x, layers=11, input_dim=2,output_dim=121, hidden_dim=300,precision=tf.float32)

    with tf.compat.v1.variable_scope('Loss'):
        
# The loss function is defined by the l2 norm of matrix (G*y-G*y_true) divided by the l2 norm of (G*y_true)
        loss = ((tf.compat.v1.losses.mean_squared_error(tf.linalg.matmul(Gram,y),tf.linalg.matmul(Gram,y_true)))/tf.linalg.norm(tf.linalg.matmul(Gram,y_true)))
    lrn_rate = 0.0002
    
# Adam Optimizer has default beta1 = 0.9, beta2 = 0.999, epsilon = 1e-8 and alpha = 0.001.
# Only alpha is different from the hyper-parameters used in the PPDE paper. Therefore, we only need to define alpha.
    opt = tf.compat.v1.train.AdamOptimizer(learning_rate=lrn_rate)
    train_op = opt.minimize(loss)
    
# This prints the details about the trainable variables which are the weight and the bias matrices. 
# By printing this, we can verify the number of layers and the size of the matrices.
    print(tf.compat.v1.trainable_variables())

    losses = []

# We start a session the train the variables.
    print(np.shape(x_train))
    with tf.compat.v1.Session() as sess:
        # init variables
        sess.run(tf.compat.v1.global_variables_initializer())

        for i in range(epochs):
          count = 0
          for x_train_batch, y_true_train_batch in get_batch(x_train, y_train, batch_size):
            count = count + 1

# We run a session with the loss function and the objective to minimize the loss function. 
# We feed x into the session to obtain y_pred.
# We then feed y_true into the session to calculate the loss function.
# We can graph the process of minimizing the loss function by keeping track of the current loss.
            current_loss, _ = sess.run([loss, train_op],
                            feed_dict={x: x_train_batch, \
                                        y_true: y_true_train_batch})
            losses.append(current_loss)
# We now have the neural network and it is named y.
        
        
        
# We run a session with the function y.
# We feed x_test into the function y, and we obtain y_pred. 
# To use the neural network, replace x_test by desired values of x. y_res[0] will give the corresponding value of y.
        y_res = sess.run([y], feed_dict = {x: x_test.reshape(2,num_test_pts)})
        y_NN = y_res[0]
        
print('done')
# x = range(epochs*(epochs/batch_size))
# This is used to graph the evolution of current_loss relative to the number of batches trained.
x = range(40000)
plt.semilogy(x,losses)
